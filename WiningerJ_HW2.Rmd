---
title: "STAT 400 HW 2"
author: ' '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter

```{r, message=F, warning=F}
library(dplyr)
library(ggplot2)
library(kableExtra)
#Add libraries if needed
```

## Problem 1

(Interaction) Recall the [gender discrimination study](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#guided-exercises) and data from HW 1:

```{r, indent = "     "}
banksalary = read.csv(file='https://raw.githubusercontent.com/proback/BeyondMLR/master/data/banksalary.csv', header=T)
```

a.  Suppose we are interested in investigating the relationships between beginning salary, experience, and age. Rather than using the employee's age directly, you should crudely classify an employee as "young" or "old" by using the median age as a breakpoint for categorizing workers. (If any employee's age matches the median, assign them to the "young" category.) Then, generate an appropriate color-coded scatterplot with trend lines and comment on whether you think there is an interaction effect between experience and age group.

```{r}
banksalary <- banksalary %>%
  mutate(ageFactor = ifelse(age <= median(banksalary$age), "young", "old"))

banksalary %>% ggplot(mapping = aes(x = exper, y = bsal, color = ageFactor))+
  geom_point()+
  geom_smooth(method = lm, se =F)+
  labs(title = "Starting Salary by Experience by Age",
       y = "Starting Salary",
       x = "Experience",
       color = "Age")
```

I believe there is an interaction between experience and age-group. The best-fit lines for the old and young groups cross, implying that the effect of experience on salary is different for the two age groups.

b.  Build a model for beginning salary that uses experience, age group (young or old), and the interaction between experience and age group. Then, interpret the partial t-test associated with the interaction.

```{r}
model1b <- lm(bsal~exper+ageFactor+exper*ageFactor, data = banksalary)

summary(model1b)
```

The partial t-test associated with the interaction has a t-value of 4.537 which corresponds to a p-value of 1.78e-05. From this, one should conclude that there is interaction between age and experience. This means that the effect of experience on salary differs by age group, which is consistent with what the scatterplot shows.

c.  Based on the partial t-test for the age group indicator in the model from Part b., explain why age group should NOT be removed from the model if the model already has experience and the experience-age group interaction even though the p-value suggests a lack of statistical significance.

Age group should not be removed from the model because even though it by itself is not significant, its interaction with experience is. When using an interaction term, all terms interacting should also remain in the model by themselves.

d.  Using the model from Part b., perform the calculations listed below. You may either perform the calculations by hand or use R to perform them. Either way, include enough work/code so that it is obvious how you are calculating the answers.

```{r}
5222.183 + 1.122*75 -399.958*1 + 14.605*1
5222.183 + 1.122*76 -399.958*1 + 14.605*1
5222.183 + 1.122*75 -399.958*0 + 14.605*0
5222.183 + 1.122*76 -399.958*0 + 14.605*0
```

```         
- Predict the beginning salary for a young employee with 75 months of experience.
$4920.98
- Predict the beginning salary for a young employee with 76 months of experience.
$4922.10
- Predict the beginning salary for an old employee with 75 months of experience.
$5306.33
- Predict the beginning salary for an old employee with 76 months of experience.
$5307.46
```

e.  What impact does an increase of 1 month of experience have on beginning salary? Explain.

An increase of 1 month of experience, assuming all else is held equal, should increase the beginning salary by \$1.12, because that is the coefficient for the experience term in the model.,

## Problem 2

(Comparing Models using Likelihoods) Recall the [gender discrimination study](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#guided-exercises) and data from HW 1:

```{r, indent = "     "}
banksalary = read.csv(file='https://raw.githubusercontent.com/proback/BeyondMLR/master/data/banksalary.csv', header=T)
```

```         
 Treating beginning salary as the response ($Y$) and denoting predictors experience, education, seniority, and sex by $x_1,\ldots,x_4$, respectively, consider the regression model shown below.
```

$$\text{Model 1: }Y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{3i}+\beta_4x_{4i}+\epsilon_i$$

```         
 This model can be fit with the following code:
```

```{r,indent = "      "}
model1 = lm(bsal~exper+educ+senior+sex,data=banksalary)
```

a.  Suppose we want to know if the model above could be reduced to $$\text{Model 2: }Y_i=\beta_0+\beta_1x_{1i}+\beta_4x_{4i}+\epsilon_i
    $$ State the hypotheses of interest in two ways: 1) in terms of the $\beta$ parameters and 2) in terms of the models (full vs reduced).
    -   Null Hypothesis: $H_0: \beta_2 = \beta_3 = 0$

    -   Alternative Hypothesis: $H_1: \beta_j \ne 0 \text{ for at least 1 } j \in \{2,3\}$

    -   Null hypothesis: Only the reduced model is necessary.

    -   Alternative Hypothesis: The full model is necessary.
b.  Use R's anova() function with both models above to carry out the ANOVA F-test of the desired hypotheses. Be sure to state the value of the test statistic, the p-value, and the conclusion in the context of the problem.

```{r}
model2 <- lm(bsal~exper+sex, data = banksalary)

anova(model1, model2)
```

The test statistic is 16.454 which corresponds to a p-value of 8.498e-07, which is very significant. We must conclude, therefore, that the full model is significantly better at predicting starting salary than the reduced model.

c.  (Manual verification of the results in b.) Note that in the anova results in R, the degrees of freedom for the error (Df.res) is the sample size minus the number of $\beta$ parameters, including the intercept but NOT the error variance $\sigma^2$). So, models with more parameters will have a lower Df.res value reported this way. Verify the anova F-test test statistic by constructing the F statistic manually (i.e., use R as a calculator and calculate the value based on the numbers shown in the anova output). Remember that R refers to Sum of Squares for Error (SSE) as Residual Sum of Squares (RSS). $$F={(SSE_{\mbox{reduced}}-SSE_{\mbox{full}})/(dfe_\mbox{reduced} - dfe_{\mbox{full}})\over MSE_{\mbox{full}}}
    $$

```{r}
((31130187 - 22657469) / (90-88)) / (sum(model1$residuals^2) / 88)
```

The F statistic is correct.

d.  An alternative approach to F-test for this situation is the likelihood-ratio test (LRT), which can be constructed from the statistic $$G^2=-2(\log L_0-\log L_1)
    $$ where $\log L_0$ and $\log L_1$ are the computed (max) log-likelihoods for the reduced and full models, respectively. For large $n$, $G^2$ has an approximate $\chi^2$ distribution with degrees of freedom equal to the difference in parameters between the full and reduced models (same as the numerator Df in the F statistic). Find the LRT test statistic above and its p-value from the approximate $\chi^2$ distribution. State a conclusion in the context of the problem.

```{r}
logLik(model2) #reduced
logLik(model1)

#Compute LRT Test stat
LRT_stat <- -2*(as.numeric(logLik(model2))-as.numeric(logLik(model1)))
LRT_stat

#Compute LRT p-value
1 - pchisq(q=LRT_stat,df=2)
```

The LRT test statistic is 29.54503 which corresponds to a p-value of 3.840425e-07. This is sufficient evidence to reject the null hypothesis and conclude that the full model is necessary.

e.  How do the results of the LRT compare with those of the F test above? The results of the LRT are similar, although the test statistic and p-value are both roughly twice as large. Both tests still make the same conclusion.

f.  The AIC for a given model is given by $$\mbox{AIC} = -2\log L_1+2(\#\mbox{parameters}),
    $$ where the number of parameters here includes the intercept $\beta_0$ AND the error variance $\sigma^2$. Why would a smaller value for AIC be preferred to a larger one? A smaller AIC is preferred because the smaller AIC is, the better the tradeoff between goodness-of-fit and complexity.

g.  Use AIC values to compare the model with all four predictors (model1) to the model with age, sex, and experience (model3).

```{r}
model3 <- lm(bsal~age+sex+exper, data=banksalary)
AIC(model1) #Full
AIC(model3) #Reduced
```

Model 1 has a lower AIC, so it is a better model.

h.  Explain why you cannot use the LRT or F-test to compare model1 and model3. You cannot use the LRT or F-test to compare model1 and model3 because model1 does not contain the age predictor, so model3 is not nested within model1.

```{r}
#Read In Dataset:
elephants <- read.csv(file = "https://raw.githubusercontent.com/proback/BeyondMLR/master/data/elephant.csv")
```

## Problem 3

1.  (Inspired by Guided Exercise 2 in the Chapter 4 Exercises) How does age affect male elephant mating patterns? An article by Poole (1989) investigated whether mating success in male elephants increases with age and whether there is a peak age for mating success. To address this question, the research team followed 41 elephants for one year and recorded both their ages and their number of matings. The data (Ramsey and Schafer 2002) is found in `elephant.csv`, which can be accessed on the book's [Github page](http://github.com/proback/BeyondMLR). The variables include:

    -   `MATINGS` = the number of matings in a given year
    -   `AGE` = the age of the elephant in years

<!-- -->

a.  Create a histogram of MATINGS. Use a very small binwidth so that the discrete nature of the distribution is apparent. Is there preliminary evidence that number of matings could be modeled as a Poisson response? Explain.

```{r}
elephants %>% ggplot(mapping=aes(x=MATINGS))+
  geom_histogram(bins=50)
```

Yes there is preliminary evidence that number of matings could be modeled as a Poisson response because the histogram looks like other Poisson distributions with lamba equal to a small number such as 2 or 3.

b.  For each age, calculate the mean number of matings. Take the log of each mean and plot it by AGE.

```{r warning = F, message = F}
matings <- elephants %>%
  group_by(AGE) %>%
  summarise(mean=mean(MATINGS))

matings <- matings%>%
  mutate(logMate = log(mean))

matings %>% ggplot(mapping=aes(x=AGE, y=logMate))+
  geom_point()+
  geom_smooth()
```

```         
-   What assumption can be assessed with this plot?
This plot assesses the linearity assumption.
-   Is there evidence of a quadratic trend on this plot?
There is slight evidence of a quadratic trend on the left side of the plot, but in general it looks pretty linear.
```

c.  In Poisson regression, there is an assumption that the mean will equal the variance for a given value of x. If we were to only use AGE for predicting MATINGS, do you believe the assumption that mean=variance is met? To answer this, “discretize” age by cutting it into intervals of length 5 beginning with 25. Then, plot the distribution of MATINGS for each AGE interval and find the mean and variance of MATINGS for each interval. Comment on whether you believe the assumption is satisfied.

    -   NOTE: When the sample size is small (here it is 41), this assumption is very difficult to assess. Even when we discreteize age, some of the age groups have so few observations that is difficult to address whether the mean=variance and/or whether the Poisson assumption is reasonable.

```{r}
cuts = cut(elephants$AGE,
           breaks=c(25,30,35,40,45,50,55))
ageGrps <- data.frame(cuts,elephants)
ggplot(data = ageGrps, aes(x = MATINGS)) +
  geom_histogram(binwidth = .25, color = "black", 
                 fill = "white") +
  facet_wrap(cuts) +
  xlab("Matings")

table1chp4<- ageGrps  %>% group_by(cuts)  %>% 
  summarise(mnNum= mean(MATINGS),varNum=var(MATINGS),n=n())
kable(table1chp4, booktabs=T, 
      caption="Compare mean and variance of matings within each age group.",
      col.names = c("Age Groups", "Mean", "Variance", "n")) %>%
  kable_styling(full_width = F)
```

I do not believe this assumption is satisfied. For the smaller age groups, variance \< mean, but for the bigger age groups, variance \> mean. Despite the fact that the assumption might not be met, I will still proceed with caution.

\

d.  Fit a Poisson regression model for MATINGS that is based on a linear term for AGE. Report the estimated equation for the log mean number of MATINGS.

```{r}
model4 = glm(MATINGS ~ AGE, family = poisson, data = elephants)
summary(model4)
```

The estimated equation is $$log(\hat{\lambda_{i}}) = -1.58201 + 0.06869x_{i, AGE}$$.

e.  Interpret the rate ratio associated with AGE in the context of the problem.
The rate ratio associated with AGE is the percent change for a one unit change in AGE.

f.  Find a 90% confidence interval for the rate ratio associated with AGE **AND** interpret in the context of the problem.

```{r}
exp(confint(model4, level = .90))
```
The 90% confidence interval for the rate ratio associated with AGE is (1.04711157, 1.0955986). In this context, we are 90% confident that the mean number of matings increases by between 4.7% and 9.6% as age increases by one year. 

g.  As age increases, does the mean number of MATINGS increase, decrease, or stay the same? Explain your reasoning. The mean number of matings increases as age increases because even the lower interval of the confidence interval of the rate ratio was over 100%, meaning they increase year-to-year.

h.  Are the number of matings significantly related to age? Test with a drop-in-deviance test using a Type I Error rate of 0.05.
```{r}
model5 <- glm(MATINGS ~ 1, family = poisson, data = elephants)

dropDev <- model5$deviance - model4$deviance

1 - pchisq(dropDev, df = 1)
```
Yes, the number of matings are significantly related to age. The p-value of 7.99062e-07 allows us to make that conclusion.

i.  Add a quadratic term in AGE to determine whether there is a maximum age for the number of matings for elephants. Is a quadratic model preferred to a linear model? Test with a drop-in-deviance test using a Type I Error rate of 0.05.

```{r}
elephants <- elephants %>% 
  mutate(AGE2=AGE^2)

model6 <- glm(MATINGS ~ AGE+AGE2, family = poisson, data = elephants)

dropDev2 <- model4$deviance - model6$deviance

1 - pchisq(dropDev2, df = 1)
```

Because the p-value of 0.6667354 is not significant, we can conclude that a quadratic model is not preferred to a linear model. According to these data, there is not a maximum age for the number of matings for elephants.

## Uploads

Please upload your .html and .Rmd files in Canvas. Name the files using LastnameFirstinitial_HW2.fileextension.
